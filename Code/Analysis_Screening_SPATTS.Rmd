---
title: "Analysis_SPATTS"
output: html_document
date: "2023-04-20"
---

## Libraries
```{r, warning = F, message = F}
library(tidyverse)
library(factoextra)
library(ggfortify)
library(caret)
library(readxl)
library(glmnet)
library(readxl)
library(pROC)
library(ggpubr) 
```

# Custom functions used in this analysis
```{r, warning = F, message = F}
bin_data <- function(temp, bin_width_mz, bin_width_drift) {
  # Create new bin labels based on column values
  column_values <- as.numeric(colnames(temp))
  bin_labels <- paste0(column_values - column_values%%bin_width_mz, "-", column_values - column_values%%bin_width_mz + bin_width_mz)
  colnames(temp) <- bin_labels
  
  # Replace all NAs with zeros
  temp[is.na(temp)] <- 0
  
  # Sum together the columns with the same name
  temp_col <- as.data.frame(t(rowsum(t(temp), group = colnames(temp), na.rm = TRUE)))
  
  # Create consistent bin sizes by summing rows with the same row name
  row_values <- as.numeric(row.names(temp_col))
  bin_labels <- paste0(row_values - row_values%%bin_width_drift, "-", row_values - row_values%%bin_width_drift + bin_width_drift)
  temp_col$rownames <- bin_labels
  
  temp_row <- aggregate(. ~ rownames, data = temp_col, FUN = sum)
  row.names(temp_row) <- temp_row$rownames
  temp_row <- temp_row[, -1]
  
  return(temp_row)
}

customSummary <- function(data, lev = NULL, model = NULL) {
  # Include twoClassSummary for Sensitivity, Specificity, etc.
  twoClass <- twoClassSummary(data, lev = lev, model = model)
  # Compute Kappa and Accuracy
  cm <- confusionMatrix(data$pred, data$obs)
  kappa <- cm$overall['Kappa']
  accuracy <- cm$overall['Accuracy']
  # Combine results into a list
  out <- c(twoClass, Kappa = kappa, Accuracy = accuracy)
  return(out)
}

stratified_sampling <- function(X, Y, size) {
  Y <- as.factor(Y)
  
  # Calculate the minimum size to ensure each class is represented
  min_size_per_class <- max(round(size / length(levels(Y))), 1)
  
  sampled_indices <- unlist(lapply(levels(Y), function(y_level) {
    indices <- which(Y == y_level)
    sample(indices, min_size_per_class, replace = TRUE)
  }))
  
  return(sampled_indices)
}

```



## Read in and format data
```{r}
file_list <- list.files("..\\Raw_data\\SPATTS\\Negative", pattern = "\\.csv$", full.names = TRUE)
meta <- read_excel("..\\Raw_data\\SPATTS\\Metadata.xlsx", sheet = 1)

# Remove filter circles
file_list <- file_list[!(str_detect(file_list, 'Filter'))]

# Clean up metadata before combination
meta_clean <- meta[-1, -(1:6)]
colnames(meta_clean) <- meta_clean[1,]
meta_clean <- meta_clean[-1,] %>%
  mutate(file = paste0(`Sample Location`, " ", `Deployment Date`, '.csv')) %>%
  select(`Weight (g)`, file) %>%
  na.omit() %>%
  mutate(file = str_replace(file, pattern = "'", replacement = ""))
meta_clean$`Weight (g)` <- as.numeric(meta_clean$`Weight (g)`)
meta_clean <- meta_clean 

# Read in all data and format
dat_list <- list()
for(i in 1:length(file_list)){
  
  # Read in file and remove irrelevant info
  temp <- read.csv(file_list[i], header= F, sep=",")
  temp <- temp[-c(1,2,3,5),-2]

  # Replace all NAs with zeros
  temp[is.na(temp)] <- 0
  
  # Move drift to be rownames and masses to colnames
  row.names(temp) <- temp[, 1]
  temp <- temp[,-1]
  colnames(temp) <- temp[1,]
  temp <- temp[-1,]
  temp <- temp[as.numeric(row.names(temp)) > 13,]
  temp <- temp[,as.numeric(colnames(temp)) <= 1000]
  temp <- temp[,as.numeric(colnames(temp)) > 99]

  
  # Bin data
  temp_row <- bin_data(temp, bin_width_mz = 2, bin_width_drift = 0.5)

  # Get weight and normalize
  name <- basename(file_list[i])
  blank_df <- meta_clean %>%
    filter(file == name)
  blank <- blank_df$`Weight (g)`
  temp_row <- temp_row %>%
    mutate(across(everything(), ~ . / blank))

  # Append to list
  dat_list[[i]] <- temp_row
  names(dat_list)[i] <- basename(file_list[i])
}

# Subtract blank from all other samples
for (i in 2:length(dat_list)) {
  dat_list[[i]] <- dat_list[[i]] - dat_list[[1]]
}
dat_list <- dat_list[-1]

# Convert to long format
dat_list_new <- list()
for (i in 1:length(dat_list)) {
  temp <- dat_list[[i]] %>%
    rownames_to_column(., 'Drift') %>%
    gather(., key = 'mz', value = intensity, 2:ncol(.)) %>%
    mutate(intensity = ifelse(intensity < 1, 1, intensity)) %>%
    mutate(intensity = log2(intensity))
  dat_list_new[[i]] <- temp
  names(dat_list_new)[i] <- names(dat_list)[i]
}


# Combine dfs and convert to wide
df <- bind_rows(dat_list_new, .id = 'file') %>%
  mutate(Group = ifelse(str_detect(.$file, 'KB'), 'Down', 'Up')) %>%
  mutate(Coord = paste0("(",mz, ",", Drift, ")"))  %>%
  select(-c('Drift', 'mz')) %>%
  spread(., key = Coord, value = intensity)
save(df, file = "..\\Data_subsets\\SPATTS\\df.Rdata")
```

## Apply abundance filter
```{r}
# load("..\\Data_subsets\\SPATTS\\df.Rdata")

# Define the range of columns for which you want to calculate summaries
start_column <- 3
end_column <- ncol(df)

# # Calculate summary statistics for each column
summary_results <- sapply(df[, start_column:end_column], function(col) {
  result <- c(
    Min = min(col, na.rm = TRUE),
    Max = max(col, na.rm = TRUE),
    Median = median(col, na.rm = TRUE),
    Mean = mean(col, na.rm = TRUE),
    SD = sd(col, na.rm = TRUE)
  )
  names(result) <- c("Min", "Max", "Median", "Mean", "SD")
  return(result)
})

# Create a dataframe from the summary results
summary_df <- as.data.frame(summary_results) %>%
  t(.) %>%
  as.data.frame()
summary(summary_df$Median)
summary(summary_df$Mean)

# Define cut off
quant <- quantile(summary_df$Median, .25)

# Subset
summary_feat <- summary_df %>%
  filter(Median >= quant)
feats <- rownames(summary_feat)
df_feat <- df[, colnames(df) %in% feats]
df_stat <- cbind(df[,1:2], df_feat)

# Save
save(df_stat, file = "..\\Data_subsets\\SPATTS\\df_stat.Rdata")
```

## Assess normality and look for outliers
```{r}
load("..\\Data_subsets\\SPATTS\\df_stat.Rdata")

# Convert to long
df_long <- df_stat %>%
  gather(key = 'coord', value = 'intensity', -(1:2))
d <- density(df_long$intensity)
plot(d)

# PCA
res.pca <- prcomp(df_stat[, -c(1:2)])
scree_plot<-fviz_eig(res.pca) #scree plot 

# Plot
pca <- autoplot(res.pca, data = df_stat, colour = 'Group') + 
  theme_classic() #+
  #geom_text(label = df_stat$file)
print(pca)


# # Hierarchical clustering
dist <- dist(df_stat[,3:ncol(df_stat)], method = 'euclidean')
hc1 <- hclust(dist, method = 'average')
plot(hc1, cex = 0.6, hang = -1)
```


# Select features
```{r}
# Load in formatted data
load("..\\Data_subsets\\SPATTS\\df_stat.Rdata")

# Define independent and dependent variables
X <- df_stat[,3:ncol(df_stat)]
Y <- df_stat$Group

# Set the seed for reproducibility
set.seed(1234)

# Split the data into training and testing sets
train_indices <- createDataPartition(Y, p = 3/4, list = FALSE)
train <- df_stat[train_indices, ]
test <- df_stat[-train_indices, ]
train_X <- X[train_indices, ]
train_Y <- Y[train_indices]
test_X <- X[-train_indices, ]
test_Y <- Y[-train_indices]

# Perform bootstrapped lasso for feature selection 
n_bootstraps <- 1000 # Number of bootstrap samples
feature_selection_frequency <- matrix(0, ncol = ncol(train_X), nrow = n_bootstraps)

# Initialize an empty list to store selected features from each iteration
selected_features_list <- vector("list", n_bootstraps)

for(i in 1:n_bootstraps) {

  cat("Bootstrap Iteration:", i, "\n")

  # Stratified sampling for bootstrap
  bootstrap_indices <- stratified_sampling(train_X, train_Y, size = nrow(train_X))
  bootstrap_X <- train_X[bootstrap_indices, ]
  bootstrap_Y <- train_Y[bootstrap_indices]

  # Create model
  cv_model <- cv.glmnet(as.matrix(bootstrap_X), bootstrap_Y, alpha = 1, family = 'binomial', type.measure = 'class')
  lasso_model <- glmnet(as.matrix(bootstrap_X), bootstrap_Y, alpha = 1, family = 'binomial', lambda = cv_model$lambda.min)
    
  # Extract coefficients and record selected features
  coef_lasso <- coef(lasso_model, s = cv_model$lambda.min)
  selected_features <- rownames(coef_lasso)[which(coef_lasso != 0)]

  # Update the list with selected features
  selected_features_list[[i]] <- selected_features
}

# Aggregate the list into a frequency table
feature_selection_table <- table(unlist(selected_features_list))

# Convert to dataframe
feature_selection_df <- data.frame(
  Feature = names(feature_selection_table),
  SelectionCount = as.integer(feature_selection_table)
)

# Select features selected at least x times 
feature_selection_df <- feature_selection_df %>%
  filter(SelectionCount > 200.
         & Feature != '(Intercept)')
top_features <- feature_selection_df$Feature

save(top_features, file = "..\\Data_subsets\\SPATTS\\top_features.Rdata")
write.csv(top_features, file = "..\\Data_subsets\\SPATTS\\top_features.csv")

# Prune data set down
load("..\\Data_subsets\\SPATTS\\top_features.Rdata")
top_train <- train[colnames(train) %in% top_features]
top_train$Group <- train$Group
top_test <- test[colnames(test) %in% top_features]
top_test$Group <- test$Group
```


# Modeling
```{r}
# Data split
X <- top_train %>%
  dplyr::select(-"Group")
Y <- as.factor(top_train$Group)
X_test <- top_test %>%
  dplyr::select(-"Group")
Y_test <- as.factor(top_test$Group)

# Set up the control with a custom summary function
ctrl <- trainControl(
  method = "cv",
  number = 5,
  summaryFunction = customSummary,
  classProbs = TRUE,  # Important for twoClassSummary
  savePredictions = TRUE
)

# Train the random forest model using the training data
rf_model <- train(
  x = X,
  y = Y,
  method = 'svmLinear',  
  trControl = ctrl,  
  tuneLength = 5,  
  metric = "ROC"  # ROC is typically used with twoClassSummary
)

# View results
print(rf_model)

# Extract feature importance
feature_importance <- varImp(rf_model) %>%
  .[[1]] %>%
  arrange(desc(Down)) 

# Obtain predictions on the testing data
predictions <- predict(rf_model, X_test)

# Create the confusion matrix
confusion_matrix <- confusionMatrix(predictions, as.factor(Y_test))

# Print the confusion matrix
print(confusion_matrix)

# Calculate accuracy from the confusion matrix
accuracy <- confusion_matrix$overall["Accuracy"]

# Calculate Cohen's Kappa from the confusion matrix
kappa <- confusion_matrix$overall["Kappa"]

# Calculate Sensitivity (True Positive Rate) from the confusion matrix
sensitivity <- confusion_matrix$byClass["Sensitivity"]

# Calculate Specificity (True Negative Rate) from the confusion matrix
specificity <- confusion_matrix$byClass["Specificity"]
```

# Make boxplots of top 5 features
```{r}
top_df <- top_train %>%
  gather(key = 'coord', value = 'intensity', -ncol(top_train)) %>%
  .[.$coord %in% rownames(feature_importance),] %>%
  mutate(Group = ifelse(Group == 'Up', 'Upstream', 'Downstream'))
top_df$coord <- factor(top_df$coord, levels = rownames(feature_importance))
top_df$Group <- factor(top_df$Group, levels = c('Upstream', 'Downstream'))


# Plot 
p <- ggplot(data = top_df, aes(x = intensity, y = coord, fill = Group)) + 
  geom_boxplot(outlier.size = 0.5, show.legend = T) +  # Decreased outlier size
  theme_classic() +
  scale_fill_manual(values = c('Upstream' = 'grey', 'Downstream' = 'darkred')) +
  theme(axis.text = element_text(size = 8),
        axis.title = element_text(size = 8, face = 'bold'),
        axis.text.x = element_text(size = 7, hjust = 1),
        axis.text.y = element_text(size = 7, hjust = 1),
        legend.position = "bottom",  # Moved legend to bottom
        legend.text = element_text(size = 7),  # Adjust the legend text size (smaller)
        legend.key.size = unit(0.5, "cm"),
        legend.title = element_text(size = 8)) +  # Adjust the legend title size
  theme(plot.title = element_text(hjust = 0.5)) + # Center title
  xlab('Abundance') + 
  ylab('Coordinate (m/z, drift time)')

ggsave('..//Analysis//SPATTS//Boxplots.png', plot = p, width = 3.25, height = 2.5)

```


# Plot heatmaps
```{r}
# Find best sample examples
top_df <- df_stat[, c('file', 'Group', '(636-638,24-24.5)')]

# Convert to long
df_long <- df_stat %>%
  gather(key = 'coord', value = 'intensity', -(1:2))

# Split coordinates
df_long <- df_long %>%
  mutate(coord_temp = gsub("\\(|\\)", "", coord)) %>%
  mutate(x_range = sub(",(.*)", "", coord_temp),
         y_range = sub(".*,", "", coord_temp),
         x_start = as.numeric(sub("-.*", "", x_range)),
         x_end = as.numeric(sub(".*-", "", x_range)),
         y_start = as.numeric(sub("-.*", "", y_range)),
         y_end = as.numeric(sub(".*-", "", y_range)))

# Identify important coordinates for circling
sig_coord <- df_long %>%
  filter(coord %in% unique(top_df$coord)) %>%
  .[,7:10] %>%
  unique()

# Condition for highlighting
highlight_x <- sig_coord$x_start + 1  # 1 is half of the width of the tile 
highlight_y <- sig_coord$y_start + 0.25  # 0.25 is half of the height of the tile 
highlight_data <- data.frame(x = highlight_x, y = highlight_y)


# Iterate through files and plot
files <- unique(df_long$file)
plot_list <- list()
for(i in 1:length(files)){
  temp <- df_long %>%
    filter(file == files[i]) 

  # Plot
  title <- paste0(unique(temp$file), ': ', unique(temp$group))

    
  p <- ggplot(temp) +
    geom_tile(aes(x = x_start + 1, y = y_start + 0.25, fill = intensity)) +
    geom_point(data = highlight_data, aes(x = x, y = y), shape = 1, color = "white", size = 1.5, stroke = 1) +
   scale_fill_gradientn(
      colors = c("black", "black", "black", "darkblue", "blue", "green", "yellow", "red"),
      limits = c(min(temp$intensity), max(temp$intensity))  # Set the color scale limits
    ) +
    labs(x = "m/z", y = "Drift time (ms)", fill = "Abundance") +
    theme_classic() +
    scale_x_continuous(expand = c(0, 0)) + 
    scale_y_continuous(expand = c(0, 0)) +  
    theme(
      axis.title.x = element_text(face = "bold.italic"),  # Make x-axis title bold and italicized
      axis.title.y = element_text(face = "bold"),  # Make y-axis title bold
      plot.title = element_text(hjust = 0.5)
    ) +
    ggtitle(`title`) 

  
    # Append to list
    plot_list[[i]] <- p
    names(plot_list)[i] <- files[i]
  }
save(plot_list, file = "..//Data_subsets//SPATTS//Plot_list.Rdata")

# Print plots to pdf
file <- paste0('..\\Analysis\\SPATTS\\heatmaps_mz.pdf')
pdf(file = file, width = 11, height = 6)
    plots<-ggarrange(plotlist = plot_list, ncol=2, nrow=2)
    print(plots)
dev.off()

```


